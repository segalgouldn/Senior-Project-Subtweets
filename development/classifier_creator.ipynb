{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn and NLTK to build a Naive Bayes Classifier that identifies subtweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "#### Use Scikit-Learn pipelines to define special features to add to a Naive Bayes Classifier\n",
    "#### Evaluate the accuracy of the classifier\n",
    "#### Maybe do it live, on a Twitter API stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods:\n",
    "#### Use the training set I made before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from time import time, sleep\n",
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import tweepy\n",
    "import nltk\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set max column width for dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/data_for_training/final_training_data/Subtweets_Classifier_Training_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training and test sets from the single training set I made before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, class_train, class_test = train_test_split(df.alleged_subtweet.tolist(), \n",
    "                                                                  df.is_subtweet.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use NLTK's tokenizer instead of Scikit's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class for distinguishing polarizing parts of speech as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TweetStats(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        first_names = [\"Aaliyah\", \"Aaron\", \"Abby\", \"Abigail\", \"Abraham\", \"Adam\",\n",
    "                       \"Addison\", \"Adrian\", \"Adriana\", \"Adrianna\", \"Aidan\", \"Aiden\",\n",
    "                       \"Alan\", \"Alana\", \"Alejandro\", \"Alex\", \"Alexa\", \"Alexander\",\n",
    "                       \"Alexandra\", \"Alexandria\", \"Alexia\", \"Alexis\", \"Alicia\", \"Allison\",\n",
    "                       \"Alondra\", \"Alyssa\", \"Amanda\", \"Amber\", \"Amelia\", \"Amy\",\n",
    "                       \"Ana\", \"Andrea\", \"Andres\", \"Andrew\", \"Angel\", \"Angela\",\n",
    "                       \"Angelica\", \"Angelina\", \"Anna\", \"Anthony\", \"Antonio\", \"Ariana\",\n",
    "                       \"Arianna\", \"Ashley\", \"Ashlyn\", \"Ashton\", \"Aubrey\", \"Audrey\",\n",
    "                       \"Austin\", \"Autumn\", \"Ava\", \"Avery\", \"Ayden\", \"Bailey\",\n",
    "                       \"Benjamin\", \"Bianca\", \"Blake\", \"Braden\", \"Bradley\", \"Brady\",\n",
    "                       \"Brandon\", \"Brayden\", \"Breanna\", \"Brendan\", \"Brian\", \"Briana\",\n",
    "                       \"Brianna\", \"Brittany\", \"Brody\", \"Brooke\", \"Brooklyn\", \"Bryan\",\n",
    "                       \"Bryce\", \"Bryson\", \"Caden\", \"Caitlin\", \"Caitlyn\", \"Caleb\",\n",
    "                       \"Cameron\", \"Camila\", \"Carlos\", \"Caroline\", \"Carson\", \"Carter\",\n",
    "                       \"Cassandra\", \"Cassidy\", \"Catherine\", \"Cesar\", \"Charles\", \"Charlotte\",\n",
    "                       \"Chase\", \"Chelsea\", \"Cheyenne\", \"Chloe\", \"Christian\", \"Christina\",\n",
    "                       \"Christopher\", \"Claire\", \"Cody\", \"Colby\", \"Cole\", \"Colin\",\n",
    "                       \"Collin\", \"Colton\", \"Conner\", \"Connor\", \"Cooper\", \"Courtney\",\n",
    "                       \"Cristian\", \"Crystal\", \"Daisy\", \"Dakota\", \"Dalton\", \"Damian\",\n",
    "                       \"Daniel\", \"Daniela\", \"Danielle\", \"David\", \"Delaney\", \"Derek\",\n",
    "                       \"Destiny\", \"Devin\", \"Devon\", \"Diana\", \"Diego\", \"Dominic\",\n",
    "                       \"Donovan\", \"Dylan\", \"Edgar\", \"Eduardo\", \"Edward\", \"Edwin\",\n",
    "                       \"Eli\", \"Elias\", \"Elijah\", \"Elizabeth\", \"Ella\", \"Ellie\", \n",
    "                       \"Emily\", \"Emma\", \"Emmanuel\", \"Eric\", \"Erica\", \"Erick\",\n",
    "                       \"Erik\", \"Erin\", \"Ethan\", \"Eva\", \"Evan\", \"Evelyn\",\n",
    "                       \"Faith\", \"Fernando\", \"Francisco\", \"Gabriel\", \"Gabriela\", \"Gabriella\",\n",
    "                       \"Gabrielle\", \"Gage\", \"Garrett\", \"Gavin\", \"Genesis\", \"George\",\n",
    "                       \"Gianna\", \"Giovanni\", \"Giselle\", \"Grace\", \"Gracie\", \"Grant\",\n",
    "                       \"Gregory\", \"Hailey\", \"Haley\", \"Hannah\", \"Hayden\", \"Hector\",\n",
    "                       \"Henry\", \"Hope\", \"Hunter\", \"Ian\", \"Isaac\", \"Isabel\",\n",
    "                       \"Isabella\", \"Isabelle\", \"Isaiah\", \"Ivan\", \"Jack\", \"Jackson\",\n",
    "                       \"Jacob\", \"Jacqueline\", \"Jada\", \"Jade\", \"Jaden\", \"Jake\",\n",
    "                       \"Jalen\", \"James\", \"Jared\", \"Jasmin\", \"Jasmine\", \"Jason\", \n",
    "                       \"Javier\", \"Jayden\", \"Jayla\", \"Jazmin\", \"Jeffrey\", \"Jenna\",\n",
    "                       \"Jennifer\", \"Jeremiah\", \"Jeremy\", \"Jesse\", \"Jessica\", \"Jesus\",\n",
    "                       \"Jillian\", \"Jocelyn\", \"Joel\", \"John\", \"Johnathan\", \"Jonah\",\n",
    "                       \"Jonathan\", \"Jordan\", \"Jordyn\", \"Jorge\", \"Jose\", \"Joseph\",\n",
    "                       \"Joshua\", \"Josiah\", \"Juan\", \"Julia\", \"Julian\", \"Juliana\",\n",
    "                       \"Justin\", \"Kaden\", \"Kaitlyn\", \"Kaleb\", \"Karen\", \"Karina\",\n",
    "                       \"Kate\", \"Katelyn\", \"Katherine\", \"Kathryn\", \"Katie\", \"Kayla\",\n",
    "                       \"Kaylee\", \"Kelly\", \"Kelsey\", \"Kendall\", \"Kennedy\", \"Kenneth\",\n",
    "                       \"Kevin\", \"Kiara\", \"Kimberly\", \"Kyle\", \"Kylee\", \"Kylie\",\n",
    "                       \"Landon\", \"Laura\", \"Lauren\", \"Layla\", \"Leah\", \"Leonardo\",\n",
    "                       \"Leslie\", \"Levi\", \"Liam\", \"Liliana\", \"Lillian\", \"Lilly\",\n",
    "                       \"Lily\", \"Lindsey\", \"Logan\", \"Lucas\", \"Lucy\", \"Luis\",\n",
    "                       \"Luke\", \"Lydia\", \"Mackenzie\", \"Madeline\", \"Madelyn\", \"Madison\",\n",
    "                       \"Makayla\", \"Makenzie\", \"Malachi\", \"Manuel\", \"Marco\", \"Marcus\",\n",
    "                       \"Margaret\", \"Maria\", \"Mariah\", \"Mario\", \"Marissa\", \"Mark\",\n",
    "                       \"Martin\", \"Mary\", \"Mason\", \"Matthew\", \"Max\", \"Maxwell\",\n",
    "                       \"Maya\", \"Mckenzie\", \"Megan\", \"Melanie\", \"Melissa\", \"Mia\",\n",
    "                       \"Micah\", \"Michael\", \"Michelle\", \"Miguel\", \"Mikayla\", \"Miranda\",\n",
    "                       \"Molly\", \"Morgan\", \"Mya\", \"Naomi\", \"Natalia\", \"Natalie\",\n",
    "                       \"Nathan\", \"Nathaniel\", \"Nevaeh\", \"Nicholas\", \"Nicolas\", \"Nicole\",\n",
    "                       \"Noah\", \"Nolan\", \"Oliver\", \"Olivia\", \"Omar\", \"Oscar\",\n",
    "                       \"Owen\", \"Paige\", \"Parker\", \"Patrick\", \"Paul\", \"Payton\",\n",
    "                       \"Peter\", \"Peyton\", \"Preston\", \"Rachel\", \"Raymond\", \"Reagan\",\n",
    "                       \"Rebecca\", \"Ricardo\", \"Richard\", \"Riley\", \"Robert\", \"Ruby\",\n",
    "                       \"Ryan\", \"Rylee\", \"Sabrina\", \"Sadie\", \"Samantha\", \"Samuel\",\n",
    "                       \"Sara\", \"Sarah\", \"Savannah\", \"Sean\", \"Sebastian\", \"Serenity\",\n",
    "                       \"Sergio\", \"Seth\", \"Shane\", \"Shawn\", \"Shelby\", \"Sierra\",\n",
    "                       \"Skylar\", \"Sofia\", \"Sophia\", \"Sophie\", \"Spencer\", \"Stephanie\",\n",
    "                       \"Stephen\", \"Steven\", \"Summer\", \"Sydney\", \"Tanner\", \"Taylor\", \n",
    "                       \"Thomas\", \"Tiffany\", \"Timothy\", \"Travis\", \"Trenton\", \"Trevor\",\n",
    "                       \"Trinity\", \"Tristan\", \"Tyler\", \"Valeria\", \"Valerie\", \"Vanessa\",\n",
    "                       \"Veronica\", \"Victor\", \"Victoria\", \"Vincent\", \"Wesley\", \"William\",\n",
    "                       \"Wyatt\", \"Xavier\", \"Zachary\", \"Zoe\", \"Zoey\"]\n",
    "        first_names_lower = set([name.lower() for name in first_names])\n",
    "\n",
    "        pronouns = [\"You\", \"You're\", \"Youre\", \"Your\", \"U\", \"Ur\",\n",
    "                    \"She\", \"She's\", \"Her\", \"Hers\", \n",
    "                    \"He\", \"He's\", \"Hes\", \"Him\", \"His\", \n",
    "                    \"They\", \"They're\", \"Theyre\", \"Them\", \"Their\", \"Theirs\"]\n",
    "        prounouns_lower = set([pronoun.lower() for pronoun in pronouns])\n",
    "        \n",
    "        first_person_pronouns = [\"I\", \"I'm\", \"Im\", \"We\", \"We're\", \"Our\", \"My\", \"Us\"]\n",
    "        first_person_pronouns_lower = set([pronoun.lower() for pronoun in first_person_pronouns])\n",
    "        \n",
    "        pattern = \"(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
    "        \n",
    "        final_output = []\n",
    "        for text in posts:\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    "            \n",
    "            num_pronouns = len(prounouns_lower.intersection(tokenized_text))\n",
    "            num_names = len(first_names_lower.intersection(tokenized_text))\n",
    "            num_first_person = len(first_person_pronouns_lower.intersection(tokenized_text))\n",
    "            num_at_symbols = text.count(\"@\")\n",
    "            num_subtweet = text.count(\"subtweet\") + text.count(\"Subtweet\")\n",
    "            num_urls = len(re.findall(pattern, text))\n",
    "            \n",
    "            \n",
    "            weighted_dict = {\"num_subtweet\": bool(num_subtweet),\n",
    "                             \"num_at_symbols\": bool(num_at_symbols), \n",
    "                             \"num_urls\": bool(num_urls),\n",
    "                             \"num_pronouns\": bool(num_pronouns),\n",
    "                             \"num_names\": bool(num_names), \n",
    "                             \"num_first_person\": bool(num_first_person)}\n",
    "            final_output.append(weighted_dict)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"ngram_tf_idf\", Pipeline([\n",
    "            (\"counts\", CountVectorizer(tokenizer=tokenizer.tokenize)),\n",
    "            (\"tf_idf\", TfidfTransformer())\n",
    "        ])),\n",
    "        (\"stats_vect\", Pipeline([\n",
    "            (\"tweet_stats\", TweetStats()),\n",
    "            (\"vect\", DictVectorizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline.fit(text_train, class_train)\n",
    "predictions = sentiment_pipeline.predict(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(class_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function for visualizing confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False,\n",
    "                          title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"negative\", \"positive\"]\n",
    "\n",
    "cnf_matrix = confusion_matrix(class_test, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the classifier for another time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sentiment_pipeline, \"../data/other_data/subtweets_classifier.pkl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print tests for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests_dataframe(tweets_dataframe, text_column=\"SentimentText\", sentiment_column=\"Sentiment\"):\n",
    "    predictions = sentiment_pipeline.predict_proba(tweets_dataframe[text_column])\n",
    "    negative_probability = predictions[:, 0].tolist()\n",
    "    positive_probability = predictions[:, 1].tolist()\n",
    "    return pd.DataFrame({\"tweet\": tweets_dataframe[text_column], \n",
    "                         \"sentiment_score\": tweets_dataframe[sentiment_column], \n",
    "                         \"subtweet_negative_probability\": negative_probability, \n",
    "                         \"subtweet_positive_probability\": positive_probability}).sort_values(by=\"subtweet_positive_probability\", \n",
    "                                                                                             ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make up some tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = [\"Some people don't know their place.\", \n",
    "               \"Isn't it funny how some people don't know their place?\", \n",
    "               \"How come you people act like this?\", \n",
    "               \"You're such a nerd.\",\n",
    "               \"I love Noah, he's so cool.\",\n",
    "               \"Who the heck is Noah?\",\n",
    "               \"This is a @NoahSegalGould subtweet. Go check out https://segal-gould.com.\", \n",
    "               \"This is a subtweet.\", \n",
    "               \"Hey @jack!\", \n",
    "               \"Hey Jack!\",\n",
    "               \"http://www.google.com\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a dataframe from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets_df = pd.DataFrame({\"Tweet\": test_tweets, \"Sentiment\": [None]*len(test_tweets)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_dataframe(test_tweets_df, text_column=\"Tweet\", sentiment_column=\"Sentiment\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on actual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df = pd.read_csv(\"../data/data_for_testing/other_data/naji_data.csv\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair some leftover HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df[\"SentimentText\"] = naji_df[\"SentimentText\"].str.replace(\"&quot;\", \"\\\"\")\n",
    "naji_df[\"SentimentText\"] = naji_df[\"SentimentText\"].str.replace(\"&amp;\", \"&\")\n",
    "naji_df[\"SentimentText\"] = naji_df[\"SentimentText\"].str.replace(\"&gt;\", \">\")\n",
    "naji_df[\"SentimentText\"] = naji_df[\"SentimentText\"].str.replace(\"&lt;\", \"<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with non-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(s):\n",
    "    return all(ord(char) < 128 for char in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df = naji_df[naji_df[\"SentimentText\"].map(is_english)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of dataset: {}\".format(len(naji_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use randomly selected 10K rows from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df = naji_df.sample(n=10000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print and time the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "naji_df = tests_dataframe(naji_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df.to_csv(\"../data/data_from_testing/other_data/naji_tests.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df_columns = [\"sentiment_score\", \"subtweet_negative_probability\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df_for_plotting = naji_df.drop(naji_df_columns, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naji_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = naji_df_for_plotting.plot.barh(logx=True, figsize=(16, 9), color=\"maroon\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Naji Dataset Randomly Selected Subtweets Test\", fontsize=18)\n",
    "ax.set_ylabel(\"Row Index\", fontsize=18);\n",
    "ax.set_xlabel(\"Subtweet Percentage Probability (logarithmic)\", fontsize=18);\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width(), i.get_y() + 0.325, \"{:.3%}\".format(i.get_width()), fontsize=10, color=\"black\")\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests on friends' tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aaron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaron_df = pd.read_csv(\"../data/data_for_testing/friends_data/akrapf96_tweets.csv\").dropna()\n",
    "aaron_df[\"Sentiment\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "aaron_df = tests_dataframe(aaron_df, text_column=\"Text\", sentiment_column=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaron_df.to_csv(\"../data/data_from_testing/friends_data/akrapf96_tests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaron_df_columns = [\"sentiment_score\", \"subtweet_negative_probability\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaron_df_for_plotting = aaron_df.drop(aaron_df_columns, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaron_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = aaron_df_for_plotting.plot.barh(logx=True, figsize=(16, 9), color=\"maroon\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Aaron Dataset Subtweets Test\", fontsize=18)\n",
    "ax.set_ylabel(\"Row Index\", fontsize=18);\n",
    "ax.set_xlabel(\"Subtweet Percentage Probability (logarithmic)\", fontsize=18);\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width(), i.get_y() + 0.325, \"{:.3%}\".format(i.get_width()), fontsize=10, color=\"black\")\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_df = pd.read_csv(\"../data/data_for_testing/friends_data/juliaeberry_tweets.csv\").dropna()\n",
    "julia_df[\"Sentiment\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "julia_df = tests_dataframe(julia_df, text_column=\"Text\", sentiment_column=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_df.to_csv(\"../data/data_from_testing/friends_data/juliaeberry_tests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_df_columns = [\"sentiment_score\", \"subtweet_negative_probability\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_df_for_plotting = julia_df.drop(julia_df_columns, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = julia_df_for_plotting.plot.barh(logx=True, figsize=(16, 9), color=\"maroon\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Julia Dataset Subtweets Test\", fontsize=18)\n",
    "ax.set_ylabel(\"Row Index\", fontsize=18);\n",
    "ax.set_xlabel(\"Subtweet Percentage Probability (logarithmic)\", fontsize=18);\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width(), i.get_y() + 0.325, \"{:.3%}\".format(i.get_width()), fontsize=10, color=\"black\")\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe_df = pd.read_csv(\"../data/data_for_testing/friends_data/zoeterhune_tweets.csv\").dropna()\n",
    "zoe_df[\"Sentiment\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zoe_df = tests_dataframe(zoe_df, text_column=\"Text\", sentiment_column=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe_df.to_csv(\"../data/data_from_testing/friends_data/zoeterhune_tests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe_df_columns = [\"sentiment_score\", \"subtweet_negative_probability\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe_df_for_plotting = zoe_df.drop(zoe_df_columns, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoe_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = zoe_df_for_plotting.plot.barh(logx=True, figsize=(16, 9), color=\"maroon\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Zoe Dataset Subtweets Test\", fontsize=18)\n",
    "ax.set_ylabel(\"Row Index\", fontsize=18);\n",
    "ax.set_xlabel(\"Subtweet Percentage Probability (logarithmic)\", fontsize=18);\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width(), i.get_y() + 0.325, \"{:.3%}\".format(i.get_width()), fontsize=10, color=\"black\")\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noah_df = pd.read_csv(\"../data/data_for_testing/friends_data/noahsegalgould_tweets.csv\").dropna()\n",
    "noah_df[\"Sentiment\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "noah_df = tests_dataframe(noah_df, text_column=\"Text\", sentiment_column=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noah_df.to_csv(\"../data/data_from_testing/friends_data/noahsegalgould_tests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noah_df_columns = [\"sentiment_score\", \"subtweet_negative_probability\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noah_df_for_plotting = noah_df.drop(noah_df_columns, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noah_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = noah_df_for_plotting.plot.barh(logx=True, figsize=(16, 9), color=\"maroon\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Noah Dataset Subtweets Test\", fontsize=18)\n",
    "ax.set_ylabel(\"Row Index\", fontsize=18);\n",
    "ax.set_xlabel(\"Subtweet Percentage Probability (logarithmic)\", fontsize=18);\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width(), i.get_y() + 0.325, \"{:.3%}\".format(i.get_width()), fontsize=10, color=\"black\")\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it in realtime\n",
    "#### Define some useful variables for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.95 # 95% positives and higher, only\n",
    "DURATION = 60*60*24*7 # 1 week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Twitter API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key, consumer_secret, access_token, access_token_secret = open(\"../../credentials.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the API credentials to connect to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, retry_delay=1, timeout=120, # 2 minutes\n",
    "                 compression=True, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtweets_live_list = []\n",
    "non_subtweets_live_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom class for streaming subtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        choices = [\"retweet\", \"like\", \"retweet and like\", \"reply\"]\n",
    "        \n",
    "        id_str = status.id_str\n",
    "        screen_name = status.user.screen_name\n",
    "        created_at = status.created_at\n",
    "        retweeted = status.retweeted\n",
    "        in_reply_to = status.in_reply_to_status_id\n",
    "        \n",
    "        text = status.text.replace(\"&quot;\", \"\\\"\").replace(\"&amp;\", \"&\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "        \n",
    "        # negative_probability = sentiment_pipeline.predict_proba([text]).tolist()[0][0]\n",
    "        positive_probability = sentiment_pipeline.predict_proba([text]).tolist()[0][1]\n",
    "        \n",
    "        row = {\"tweet\": text, \n",
    "               \"screen_name\": screen_name, \n",
    "               \"time\": created_at, \n",
    "               \"subtweet_probability\": positive_probability}\n",
    "        \n",
    "        print_list = pd.DataFrame([row]).values.tolist()[0]\n",
    "        \n",
    "        if all([positive_probability >= THRESHOLD, \n",
    "                not retweeted,\n",
    "                \"RT @\" not in text, \n",
    "                not in_reply_to]):\n",
    "            \n",
    "            decision = choice(choices)\n",
    "            if decision == \"retweet\":\n",
    "                api.update_status((\"Is this a subtweet? {:.3%} \\n\" + \n",
    "                                   \"https://twitter.com/{}/status/{}\").format(positive_probability, \n",
    "                                                                              screen_name, \n",
    "                                                                              id_str))\n",
    "            \n",
    "            if decision == \"like\":\n",
    "                api.create_favorite(id_str)\n",
    "            \n",
    "            if decision == \"retweet and like\":\n",
    "                api.update_status((\"Is this a subtweet? {:.3%} \\n\" + \n",
    "                                   \"https://twitter.com/{}/status/{}\").format(positive_probability, \n",
    "                                                                              screen_name, \n",
    "                                                                              id_str))\n",
    "                api.create_favorite(id_str)\n",
    "            \n",
    "            if decision == \"reply\":\n",
    "                api.update_status(\"@{} Is this a subtweet? {:.3%}\".format(screen_name, positive_probability), id_str)\n",
    "            \n",
    "            subtweets_live_list.append(row)\n",
    "            subtweets_df = pd.DataFrame(subtweets_live_list).sort_values(by=\"subtweet_probability\", \n",
    "                                                                         ascending=False)\n",
    "            \n",
    "            subtweets_df.to_csv(\"../data/data_from_testing/live_downloaded_data/subtweets_live_data.csv\")\n",
    "            \n",
    "            print((\"Subtweet from @{0} (Probability of {1:.3%}):\\n\" + \n",
    "                   \"Time: {2}\\n\" + \n",
    "                   \"Tweet: {3}\\n\" +\n",
    "                   \"Total tweets acquired: {4}\\n\").format(print_list[0], \n",
    "                                                          print_list[1], \n",
    "                                                          print_list[2],\n",
    "                                                          print_list[3],\n",
    "                                                          (len(subtweets_live_list) \n",
    "                                                           + len(non_subtweets_live_list))))\n",
    "            \n",
    "            return row\n",
    "        else:\n",
    "            non_subtweets_live_list.append(row)\n",
    "            non_subtweets_df = pd.DataFrame(non_subtweets_live_list).sort_values(by=\"subtweet_probability\", \n",
    "                                                                                 ascending=False)\n",
    "            non_subtweets_df.to_csv(\"../data/data_from_testing/live_downloaded_data/non_subtweets_live_data.csv\")\n",
    "            \n",
    "            return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of the IDs of all my mutuals and my mutuals' followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutuals():\n",
    "    my_followers = [str(user_id) for ids_list in \n",
    "                    tweepy.Cursor(api.followers_ids, \n",
    "                                  screen_name=\"NoahSegalGould\").pages() \n",
    "                    for user_id in ids_list]\n",
    "    my_followeds = [str(user_id) for ids_list in \n",
    "                   tweepy.Cursor(api.friends_ids, \n",
    "                                 screen_name=\"NoahSegalGould\").pages() \n",
    "                   for user_id in ids_list]\n",
    "    \n",
    "    my_mutuals = list(set(my_followers) & set(my_followeds))\n",
    "    \n",
    "    bots = [\"890031065057853440\", \"895685688582180864\", \n",
    "            \"894658603977777152\", \"970553455709446144\", \n",
    "            \"786489395519983617\", \"975981192817373184\"]\n",
    "    \n",
    "    my_mutuals = [m for m in my_mutuals if m not in bots]\n",
    "    \n",
    "    with open(\"../data/other_data/NoahSegalGould_Mutuals_ids.json\", \"w\") as outfile:\n",
    "        json.dump(my_mutuals, outfile)\n",
    "    \n",
    "    return my_mutuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutuals_and_mutuals_mutuals_ids(mutuals_threshold=250):\n",
    "    my_mutuals = get_mutuals()\n",
    "    my_mutuals_mutuals = my_mutuals[:]\n",
    "\n",
    "    for i, mutual in enumerate(my_mutuals):\n",
    "        start_time = time()\n",
    "        user = api.get_user(user_id=mutual)\n",
    "        name = user.screen_name\n",
    "        is_protected = user.protected\n",
    "        if not is_protected:\n",
    "            mutuals_followers = []\n",
    "            followers_cursor = tweepy.Cursor(api.followers_ids, user_id=mutual).items()\n",
    "            while True:\n",
    "                try:\n",
    "                    mutuals_follower = followers_cursor.next()\n",
    "                    mutuals_followers.append(str(mutuals_follower))\n",
    "                except tweepy.TweepError:\n",
    "                    sleep(30) # 30 seconds\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            mutuals_followeds = []\n",
    "            followeds_cursor = tweepy.Cursor(api.friends_ids, user_id=mutual).items()\n",
    "            while True:\n",
    "                try:\n",
    "                    mutuals_followed = followeds_cursor.next()\n",
    "                    mutuals_followeds.append(str(mutuals_followed))\n",
    "                except tweepy.TweepError:\n",
    "                    sleep(30) # 30 seconds\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            mutuals_mutuals = list(set(mutuals_followers) & set(mutuals_followeds))\n",
    "            print(\"{} mutuals for mutual {}: {}\".format(len(mutuals_mutuals), i+1, name))\n",
    "            if len(mutuals_mutuals) <= mutuals_threshold: # Ignore my mutuals if they have a lot of mutuals\n",
    "                my_mutuals_mutuals.extend(mutuals_mutuals)\n",
    "            else:\n",
    "                print(\"\\tSkipping: {}\".format(name))\n",
    "        else:\n",
    "            continue\n",
    "        end_time = time()\n",
    "        with open(\"../data/other_data/NoahSegalGould_Mutuals_and_Mutuals_Mutuals_ids.json\", \"w\") as outfile:\n",
    "            json.dump(my_mutuals_mutuals, outfile)\n",
    "        print(\"{0:.2f} seconds for getting the mutuals' IDs of mutual {1}: {2}\\n\".format((end_time - start_time), \n",
    "                                                                                         i+1, name))\n",
    "    my_mutuals_mutuals = [str(mu) for mu in sorted([int(m) for m in list(set(my_mutuals_mutuals))])]\n",
    "    with open(\"../data/other_data/NoahSegalGould_Mutuals_and_Mutuals_Mutuals_ids.json\", \"w\") as outfile:\n",
    "        json.dump(my_mutuals_mutuals, outfile, indent=4)\n",
    "    return my_mutuals_mutuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# my_mutuals_mutuals = get_mutuals_and_mutuals_mutuals_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "my_mutuals = get_mutuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mutuals_mutuals = json.load(open(\"../data/other_data/NoahSegalGould_Mutuals_and_Mutuals_Mutuals_ids.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of my mutuals: {}\".format(len(my_mutuals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of my mutuals and my mutuals' mutuals: {}\".format(len(my_mutuals_mutuals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_listener = StreamListener()\n",
    "stream = tweepy.Stream(auth=api.auth, listener=stream_listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stream asynchronously, and stop it after some duration of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# bounding_box = [-73.920176, 42.009637,\n",
    "#                 -73.899739, 42.033421]\n",
    "# stream.filter(locations=bounding_box, async=True) # Bard College\n",
    "stream.filter(follow=my_mutuals_mutuals, stall_warnings=True, languages=[\"en\"], async=True)\n",
    "print(\"Streaming has started.\")\n",
    "sleep(DURATION)\n",
    "stream.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtweets_df = pd.read_csv(\"../data/data_from_testing/live_downloaded_data/subtweets_live_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtweets_df_columns = [\"screen_name\", \"time\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtweets_df_for_plotting = subtweets_df.drop(subtweets_df_columns, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subtweets_df_for_plotting.plot.barh(logx=True, figsize=(16, 9), color=\"maroon\", fontsize=13);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Live Downloaded Subtweets Test\", fontsize=18)\n",
    "ax.set_ylabel(\"Row Index\", fontsize=18);\n",
    "ax.set_xlabel(\"Subtweet Percentage Probability (logarithmic)\", fontsize=18);\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width(), i.get_y() + 0.325, \"{:.3%}\".format(i.get_width()), fontsize=10, color=\"black\")\n",
    "ax.invert_yaxis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
