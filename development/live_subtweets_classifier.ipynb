{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script for running a Twitter bot that interacts with subtweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import enchant\n",
    "import tweepy\n",
    "import nltk\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the probability threshold for interacting with a potential subtweet and the duration for which the bot should run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.75 # 75% positives and higher, only\n",
    "DURATION = 60*15 # 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up regular expressions for genericizing extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_pattern = re.compile(r'(\\#[a-zA-Z0-9]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_pattern = re.compile(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019]))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_mentions_pattern = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-\\.]))@([A-Za-z0-9_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the classifier pipeline which was previously saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = joblib.load(\"../data/other_data/subtweets_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Twitter API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key, consumer_secret, access_token, access_token_secret = open(\"../../credentials.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, retry_delay=1, timeout=120, # 2 minutes\n",
    "                 compression=True,\n",
    "                 wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lists to fill with tweets while the bot is streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtweets_live_list = []\n",
    "non_subtweets_live_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pyenchant to check if words are English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dict = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use NLTK for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom StreamListener class for use with Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        choices = [\"retweet\", \"like\", \"retweet and like\", \"reply\"]\n",
    "        \n",
    "        id_str = status.id_str\n",
    "        screen_name = status.user.screen_name\n",
    "        created_at = status.created_at\n",
    "        retweeted = status.retweeted\n",
    "        in_reply_to = status.in_reply_to_status_id_str\n",
    "        \n",
    "        # The text and media of the tweet vary based on if it's extended or not\n",
    "        if \"extended_tweet\" in status._json:\n",
    "            if \"full_text\" in status._json[\"extended_tweet\"]:\n",
    "                text = status._json[\"extended_tweet\"][\"full_text\"]\n",
    "                has_media = \"media\" in status._json[\"extended_tweet\"][\"entities\"]\n",
    "            else:\n",
    "                pass # Something else?\n",
    "        elif \"text\" in status._json:\n",
    "            text = status._json[\"text\"]\n",
    "            has_media = \"media\" in status._json[\"entities\"]\n",
    "        \n",
    "        # Genericize extra features and clean up the text\n",
    "        text = (hashtags_pattern.sub(\"➊\", \n",
    "                urls_pattern.sub(\"➋\", \n",
    "                at_mentions_pattern.sub(\"➌\",\n",
    "                text)))\n",
    "                .replace(\"\\u2018\", \"'\")\n",
    "                .replace(\"\\u2019\", \"'\")\n",
    "                .replace(\"\\u201c\", \"\\\"\")\n",
    "                .replace(\"\\u201d\", \"\\\"\")\n",
    "                .replace(\"&quot;\", \"\\\"\")\n",
    "                .replace(\"&amp;\", \"&\")\n",
    "                .replace(\"&gt;\", \">\")\n",
    "                .replace(\"&lt;\", \"<\"))\n",
    "        \n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        english_tokens = [english_dict.check(token) for token in tokens]\n",
    "        percent_english_words = sum(english_tokens)/len(english_tokens)\n",
    "        \n",
    "        # Make sure the tweet is mostly english\n",
    "        is_mostly_english = False\n",
    "        if percent_english_words >= 0.5:\n",
    "            is_mostly_english = True\n",
    "        \n",
    "        # Calculate the probability using the pipeline\n",
    "        positive_probability = sentiment_pipeline.predict_proba([text]).tolist()[0][1]\n",
    "        \n",
    "        row = {\"tweet\": text, \n",
    "               \"screen_name\": screen_name, \n",
    "               \"time\": created_at, \n",
    "               \"subtweet_probability\": positive_probability}\n",
    "        \n",
    "        print_list = pd.DataFrame([row]).values.tolist()[0]\n",
    "        \n",
    "        # Only treat it as a subtweet if all conditions are met\n",
    "        if all([positive_probability >= THRESHOLD, \n",
    "                \"RT \" != text[:3], is_mostly_english,\n",
    "                not retweeted, not in_reply_to, not has_media]):\n",
    "            \n",
    "            decision = choice(choices)\n",
    "            if decision == \"retweet\":\n",
    "                api.update_status((\"Is this a subtweet? {:.3%} \\n\" + \n",
    "                                   \"https://twitter.com/{}/status/{}\").format(positive_probability, \n",
    "                                                                              screen_name, \n",
    "                                                                              id_str))\n",
    "                print(\"Retweet!\")\n",
    "            \n",
    "            elif decision == \"like\":\n",
    "                api.create_favorite(id_str)\n",
    "                print(\"Like!\")\n",
    "            \n",
    "            elif decision == \"retweet and like\":\n",
    "                api.update_status((\"Is this a subtweet? {:.3%} \\n\" + \n",
    "                                   \"https://twitter.com/{}/status/{}\").format(positive_probability, \n",
    "                                                                              screen_name, \n",
    "                                                                              id_str))\n",
    "                api.create_favorite(id_str)\n",
    "                print(\"Retweet and like!\")\n",
    "            \n",
    "            elif decision == \"reply\":\n",
    "                api.update_status(\"@{} Is this a subtweet? {:.3%}\".format(screen_name, \n",
    "                                                                          positive_probability), \n",
    "                                  id_str)\n",
    "                print(\"Reply!\")\n",
    "            \n",
    "            subtweets_live_list.append(row)\n",
    "            subtweets_df = pd.DataFrame(subtweets_live_list).sort_values(by=\"subtweet_probability\", \n",
    "                                                                         ascending=False)\n",
    "            \n",
    "            subtweets_df.to_csv(\"../data/data_from_testing/live_downloaded_data/subtweets_live_data.csv\")\n",
    "            \n",
    "            print((\"Subtweet from @{0} (Probability of {1:.3%}):\\n\" + \n",
    "                   \"Time: {2}\\n\" + \n",
    "                   \"Tweet: {3}\\n\" +\n",
    "                   \"Total tweets acquired: {4}\\n\").format(print_list[0], \n",
    "                                                          print_list[1], \n",
    "                                                          print_list[2],\n",
    "                                                          print_list[3],\n",
    "                                                          (len(subtweets_live_list) \n",
    "                                                           + len(non_subtweets_live_list))))\n",
    "            \n",
    "            return row\n",
    "        else:\n",
    "            non_subtweets_live_list.append(row)\n",
    "            non_subtweets_df = pd.DataFrame(non_subtweets_live_list).sort_values(by=\"subtweet_probability\", \n",
    "                                                                                 ascending=False)\n",
    "            non_subtweets_df.to_csv(\"../data/data_from_testing/live_downloaded_data/non_subtweets_live_data.csv\")\n",
    "            \n",
    "            return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function for downloading IDs if users I follow who also follow me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutuals():\n",
    "    my_followers = [str(user_id) for ids_list in \n",
    "                    tweepy.Cursor(api.followers_ids, \n",
    "                                  screen_name=\"NoahSegalGould\").pages() \n",
    "                    for user_id in ids_list]\n",
    "    my_followeds = [str(user_id) for ids_list in \n",
    "                   tweepy.Cursor(api.friends_ids, \n",
    "                                 screen_name=\"NoahSegalGould\").pages() \n",
    "                   for user_id in ids_list]\n",
    "    \n",
    "    my_mutuals = list(set(my_followers) & set(my_followeds))\n",
    "    \n",
    "    bots = [\"890031065057853440\", \"895685688582180864\", \n",
    "            \"894658603977777152\", \"970553455709446144\", \n",
    "            \"786489395519983617\", \"975981192817373184\"]\n",
    "    \n",
    "    # Remove known twitter bots\n",
    "    my_mutuals = [m for m in my_mutuals if m not in bots]\n",
    "    \n",
    "    with open(\"../data/other_data/NoahSegalGould_Mutuals_ids.json\", \"w\") as outfile:\n",
    "        json.dump(my_mutuals, outfile, sort_keys=True, indent=4)\n",
    "    \n",
    "    return my_mutuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function for downloading IDs of users who follow my mutuals who are also followed by my mutuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutuals_and_mutuals_mutuals_ids(mutuals_threshold=250):\n",
    "    my_mutuals = get_mutuals()\n",
    "    my_mutuals_mutuals = my_mutuals[:]\n",
    "\n",
    "    for i, mutual in enumerate(my_mutuals):\n",
    "        start_time = time()\n",
    "        user = api.get_user(user_id=mutual)\n",
    "        name = user.screen_name\n",
    "        is_protected = user.protected\n",
    "        if not is_protected:\n",
    "            mutuals_followers = []\n",
    "            followers_cursor = tweepy.Cursor(api.followers_ids, user_id=mutual).items()\n",
    "            while True:\n",
    "                try:\n",
    "                    mutuals_follower = followers_cursor.next()\n",
    "                    mutuals_followers.append(str(mutuals_follower))\n",
    "                except tweepy.TweepError:\n",
    "                    sleep(30) # 30 seconds\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            mutuals_followeds = []\n",
    "            followeds_cursor = tweepy.Cursor(api.friends_ids, user_id=mutual).items()\n",
    "            while True:\n",
    "                try:\n",
    "                    mutuals_followed = followeds_cursor.next()\n",
    "                    mutuals_followeds.append(str(mutuals_followed))\n",
    "                except tweepy.TweepError:\n",
    "                    sleep(30) # 30 seconds\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            mutuals_mutuals = list(set(mutuals_followers) & set(mutuals_followeds))\n",
    "            print(\"{} mutuals for mutual {}: {}\".format(len(mutuals_mutuals), i+1, name))\n",
    "            if len(mutuals_mutuals) <= mutuals_threshold: # Ignore my mutuals if they have a lot of mutuals\n",
    "                my_mutuals_mutuals.extend(mutuals_mutuals)\n",
    "            else:\n",
    "                print(\"\\tSkipping: {}\".format(name))\n",
    "        else:\n",
    "            continue\n",
    "        end_time = time()\n",
    "        with open(\"../data/other_data/NoahSegalGould_Mutuals_and_Mutuals_Mutuals_ids.json\", \"w\") as outfile:\n",
    "            json.dump(my_mutuals_mutuals, outfile, sort_keys=True, indent=4)\n",
    "        print((\"{0:.2f} seconds for getting the mutuals' IDs of mutual {1}: {2}\\n\")\n",
    "              .format((end_time - start_time), i+1, name))\n",
    "    my_mutuals_mutuals = [str(mu) for mu in sorted([int(m) for m in list(set(my_mutuals_mutuals))])]\n",
    "    with open(\"../data/other_data/NoahSegalGould_Mutuals_and_Mutuals_Mutuals_ids.json\", \"w\") as outfile:\n",
    "        json.dump(my_mutuals_mutuals, outfile, indent=4)\n",
    "    return my_mutuals_mutuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# my_mutuals_mutuals = get_mutuals_and_mutuals_mutuals_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the IDs JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mutuals_mutuals = json.load(open(\"../data/other_data/NoahSegalGould_Mutuals_and_Mutuals_Mutuals_ids.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of my mutuals and my mutuals' mutuals: 4218\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of my mutuals and my mutuals' mutuals: {}\".format(len(my_mutuals_mutuals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_listener = StreamListener()\n",
    "stream = tweepy.Stream(auth=api.auth, listener=stream_listener, tweet_mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming has started.\n",
      "Retweet!\n",
      "Subtweet from @fka_zigs (Probability of 75.831%):\n",
      "Time: 2018-04-23 02:28:26\n",
      "Tweet: my mom thinks everyone on the internet is a catfish\n",
      "Total tweets acquired: 872\n",
      "\n",
      "Retweet!\n",
      "Subtweet from @JillMurphy0421 (Probability of 77.362%):\n",
      "Time: 2018-04-23 02:29:35\n",
      "Tweet: get you a roommate who wallows in mutual lonesome with you like mine does\n",
      "Total tweets acquired: 951\n",
      "\n",
      "CPU times: user 22.1 s, sys: 3.57 s, total: 25.7 s\n",
      "Wall time: 15min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# stream.filter(locations=[-73.920176, 42.009637, -73.899739, 42.033421], \n",
    "# stall_warnings=True, languages=[\"en\"], async=True)\n",
    "stream.filter(follow=my_mutuals_mutuals, stall_warnings=True, languages=[\"en\"], async=True)\n",
    "print(\"Streaming has started.\")\n",
    "sleep(DURATION)\n",
    "stream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
